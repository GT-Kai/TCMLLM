{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa7d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download, AutoTokenizer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import swanlab\n",
    "\n",
    "# ----------------- 基本配置 -----------------\n",
    "os.environ[\"SWANLAB_PROJECT\"] = \"qwen3-sft-medical\"\n",
    "\n",
    "PROMPT = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "swanlab.config.update(\n",
    "    {\n",
    "        \"model\": \"Qwen/Qwen3-1.7B\",\n",
    "        \"prompt\": PROMPT,\n",
    "        \"data_max_length\": MAX_LENGTH,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------- 数据集转换 -----------------\n",
    "def dataset_jsonl_transfer(origin_path: str, new_path: str):\n",
    "    \"\"\"\n",
    "    将原始数据集转换为大模型微调所需数据格式的新数据集\n",
    "    原始每行数据格式:\n",
    "    {\n",
    "        \"question\": \"...\",\n",
    "        \"think\": \"...\",\n",
    "        \"answer\": \"...\"\n",
    "    }\n",
    "    转换后每行:\n",
    "    {\n",
    "        \"instruction\": PROMPT,\n",
    "        \"input\": question,\n",
    "        \"output\": \"<think>...</think>\\\\n...\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # 读取旧的 JSONL 文件\n",
    "    with open(origin_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            data = json.loads(line)\n",
    "\n",
    "            user_question = data[\"question\"]\n",
    "            # 修复字符串嵌套引号问题\n",
    "            output = f\"<think>{data['think']}</think>\\n{data['answer']}\"\n",
    "\n",
    "            message = {\n",
    "                \"instruction\": PROMPT,\n",
    "                \"input\": user_question,\n",
    "                \"output\": output,\n",
    "            }\n",
    "            messages.append(message)\n",
    "\n",
    "    # 保存重构后的 JSONL 文件\n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for message in messages:\n",
    "            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# ----------------- 预处理函数 -----------------\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    将数据集进行预处理：\n",
    "    - 构造 system + user + assistant 的 prompt\n",
    "    - 拼接 input_ids / attention_mask / labels\n",
    "    \"\"\"\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    # chat 模板：system + user + assistant\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\n{PROMPT}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{example['input']}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    response = tokenizer(example[\"output\"], add_special_tokens=False)\n",
    "\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [\n",
    "        tokenizer.pad_token_id\n",
    "    ]\n",
    "    attention_mask = (\n",
    "        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    )\n",
    "    # 只训练 assistant 部分的 token\n",
    "    labels = (\n",
    "        [-100] * len(instruction[\"input_ids\"])\n",
    "        + response[\"input_ids\"]\n",
    "        + [tokenizer.pad_token_id]\n",
    "    )\n",
    "\n",
    "    # 长度截断\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------- 推理函数 -----------------\n",
    "def predict(messages, model, tokenizer):\n",
    "    \"\"\"\n",
    "    使用 chat_template 做推理\n",
    "    messages 示例:\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"...\" }\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # 使用模型的 device，避免 device_map=\"auto\" 时冲突\n",
    "    device = model.device\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    # 只保留新生成的部分\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids) :]\n",
    "        for input_ids, output_ids in zip(\n",
    "            model_inputs.input_ids, generated_ids\n",
    "        )\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )[0]\n",
    "    return response\n",
    "\n",
    "\n",
    "# ----------------- 模型加载 -----------------\n",
    "# 在 modelscope 上下载 Qwen 模型到本地目录\n",
    "model_dir = snapshot_download(\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "    cache_dir=\"/root/autodl-tmp/\",\n",
    "    revision=\"master\",\n",
    ")\n",
    "\n",
    "# Transformers 加载 tokenizer 和模型权重\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 如果没有 pad_token，则设置为 eos_token，避免 padding 报错\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 开启梯度检查点时的一些建议设置\n",
    "model.enable_input_require_grads()\n",
    "if hasattr(model, \"config\"):\n",
    "    model.config.use_cache = False  # 与 gradient_checkpointing 兼容\n",
    "\n",
    "# ----------------- 加载、处理数据集 -----------------\n",
    "train_dataset_path = \"train.jsonl\"\n",
    "test_dataset_path = \"val.jsonl\"\n",
    "\n",
    "train_jsonl_new_path = \"train_format.jsonl\"\n",
    "test_jsonl_new_path = \"val_format.jsonl\"\n",
    "\n",
    "# 首次运行时转换数据格式\n",
    "if not os.path.exists(train_jsonl_new_path):\n",
    "    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\n",
    "\n",
    "if not os.path.exists(test_jsonl_new_path):\n",
    "    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)\n",
    "\n",
    "# 得到训练集\n",
    "train_df = pd.read_json(train_jsonl_new_path, lines=True)\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "train_dataset = train_ds.map(\n",
    "    process_func, remove_columns=train_ds.column_names\n",
    ")\n",
    "\n",
    "# 得到验证集\n",
    "eval_df = pd.read_json(test_jsonl_new_path, lines=True)\n",
    "eval_ds = Dataset.from_pandas(eval_df)\n",
    "eval_dataset = eval_ds.map(\n",
    "    process_func, remove_columns=eval_ds.column_names\n",
    ")\n",
    "\n",
    "# ----------------- 训练配置 -----------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/output/Qwen3-1.7B\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",  # 修复: eval_strategy -> evaluation_strategy\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=400,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"swanlab\",\n",
    "    run_name=\"qwen3-1.7B\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ----------------- 开始训练 -----------------\n",
    "trainer.train()\n",
    "\n",
    "# ----------------- 简单主观测试 -----------------\n",
    "test_df = pd.read_json(test_jsonl_new_path, lines=True)[:3]\n",
    "\n",
    "test_text_list = []\n",
    "for _, row in test_df.iterrows():\n",
    "    instruction = row[\"instruction\"]\n",
    "    input_value = row[\"input\"]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": input_value},\n",
    "    ]\n",
    "\n",
    "    response = predict(messages, model, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        f\"Question: {input_value}\\n\"\n",
    "        f\"LLM: {response}\\n\"\n",
    "    )\n",
    "\n",
    "    test_text_list.append(swanlab.Text(response_text))\n",
    "    print(response_text)\n",
    "\n",
    "swanlab.log({\"Prediction\": test_text_list})\n",
    "swanlab.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4e9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa50f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba399bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326f750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCMLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
