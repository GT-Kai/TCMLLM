{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffafc125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /home/lick/tools/anaconda3/envs/TCMLLM/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lick/tools/anaconda3/envs/TCMLLM/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/lick/tools/anaconda3/envs/TCMLLM did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/lick/tools/anaconda3/envs/TCMLLM/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/lick/.cache/dotnet_bundle_extract')}\n",
      "  warn(msg)\n",
      "/home/lick/tools/anaconda3/envs/TCMLLM/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"en\",\"availableLanguages\"'), PosixPath('{}}'), PosixPath('\"/home/lick/.cursor-server/bin/505046dcfad2acda3d066e32b7cd8b6e2dc1fdc0/out/nls.messages.json\",\"locale\"'), PosixPath('\"en\",\"osLocale\"'), PosixPath('{\"userLocale\"'), PosixPath('\"en\",\"resolvedLanguage\"'), PosixPath('\"en\",\"defaultMessagesFile\"')}\n",
      "  warn(msg)\n",
      "/home/lick/tools/anaconda3/envs/TCMLLM/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/lick/tools/anaconda3/envs/TCMLLM/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/home/lick/tools/anaconda3/envs/TCMLLM/etc/xml/catalog file')}\n",
      "  warn(msg)\n",
      "/home/lick/tools/anaconda3/envs/TCMLLM/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download, AutoTokenizer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import swanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa7d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- 数据集转换 -----------------\n",
    "def dataset_jsonl_transfer(origin_path: str, new_path: str):\n",
    "    \"\"\"\n",
    "    将原始数据集转换为大模型微调所需数据格式的新数据集\n",
    "    原始每行数据格式:\n",
    "    {\n",
    "        \"question\": \"...\",\n",
    "        \"think\": \"...\",\n",
    "        \"answer\": \"...\"\n",
    "    }\n",
    "    转换后每行:\n",
    "    {\n",
    "        \"instruction\": PROMPT,\n",
    "        \"input\": question,\n",
    "        \"output\": \"<think>...</think>\\\\n...\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # 读取旧的 JSONL 文件\n",
    "    with open(origin_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            data = json.loads(line)\n",
    "\n",
    "            user_question = data[\"question\"]\n",
    "            # 修复字符串嵌套引号问题\n",
    "            output = f\"<think>{data['think']}</think>\\n{data['answer']}\"\n",
    "\n",
    "            message = {\n",
    "                \"instruction\": PROMPT,\n",
    "                \"input\": user_question,\n",
    "                \"output\": output,\n",
    "            }\n",
    "            messages.append(message)\n",
    "\n",
    "    # 保存重构后的 JSONL 文件\n",
    "    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for message in messages:\n",
    "            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# ----------------- 预处理函数 -----------------\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    将数据集进行预处理：\n",
    "    - 构造 system + user + assistant 的 prompt\n",
    "    - 拼接 input_ids / attention_mask / labels\n",
    "    \"\"\"\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "\n",
    "    # chat 模板：system + user + assistant\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\n{PROMPT}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{example['input']}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    response = tokenizer(example[\"output\"], add_special_tokens=False)\n",
    "\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [\n",
    "        tokenizer.pad_token_id\n",
    "    ]\n",
    "    attention_mask = (\n",
    "        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    )\n",
    "    # 只训练 assistant 部分的 token\n",
    "    labels = (\n",
    "        [-100] * len(instruction[\"input_ids\"])\n",
    "        + response[\"input_ids\"]\n",
    "        + [tokenizer.pad_token_id]\n",
    "    )\n",
    "\n",
    "    # 长度截断\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------- 推理函数 -----------------\n",
    "def predict(messages, model, tokenizer):\n",
    "    \"\"\"\n",
    "    使用 chat_template 做推理\n",
    "    messages 示例:\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"...\" }\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # 使用模型的 device，避免 device_map=\"auto\" 时冲突\n",
    "    device = model.device\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    # 只保留新生成的部分\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids) :]\n",
    "        for input_ids, output_ids in zip(\n",
    "            model_inputs.input_ids, generated_ids\n",
    "        )\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )[0]\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe25b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- 基本配置 -----------------\n",
    "os.environ[\"SWANLAB_PROJECT\"] = \"qwen3-sft-medical\"\n",
    "os.environ[\"SWANLAB_API\"] = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "PROMPT = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "swanlab.config.update(\n",
    "    {\n",
    "        \"model\": \"Qwen/Qwen3-1.7B\",\n",
    "        \"prompt\": PROMPT,\n",
    "        \"data_max_length\": MAX_LENGTH,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af4e9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: ./../BaseModels/Qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 16:39:36,882 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "2025-12-01 16:39:37,063 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# ----------------- 模型加载 -----------------\n",
    "# 在 modelscope 上下载 Qwen 模型到本地目录\n",
    "model_dir = snapshot_download(\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "    cache_dir=\"./../BaseModels/\",\n",
    "    revision=\"master\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa50f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers 加载 tokenizer 和模型权重\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 如果没有 pad_token，则设置为 eos_token，避免 padding 报错\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba399bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f286ee1eb4ec427e9cb4051755d8f41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 与 gradient_checkpointing 兼容\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mx\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 开启梯度检查点时的一些建议设置\n",
    "model.enable_input_require_grads()\n",
    "if hasattr(model, \"config\"):\n",
    "    model.config.use_cache = False  # 与 gradient_checkpointing 兼容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- 加载、处理数据集 -----------------\n",
    "train_dataset_path = \"./../datas/delicate_medical_r1_data/train.jsonl\"\n",
    "test_dataset_path = \"./../datas/delicate_medical_r1_data/val.jsonl\"\n",
    "\n",
    "train_jsonl_new_path = \"train_format.jsonl\"\n",
    "test_jsonl_new_path = \"val_format.jsonl\"\n",
    "\n",
    "# 首次运行时转换数据格式\n",
    "if not os.path.exists(train_jsonl_new_path):\n",
    "    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\n",
    "\n",
    "if not os.path.exists(test_jsonl_new_path):\n",
    "    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)\n",
    "\n",
    "# 得到训练集\n",
    "train_df = pd.read_json(train_jsonl_new_path, lines=True)\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "train_dataset = train_ds.map(\n",
    "    process_func, remove_columns=train_ds.column_names\n",
    ")\n",
    "\n",
    "# 得到验证集\n",
    "eval_df = pd.read_json(test_jsonl_new_path, lines=True)\n",
    "eval_ds = Dataset.from_pandas(eval_df)\n",
    "eval_dataset = eval_ds.map(\n",
    "    process_func, remove_columns=eval_ds.column_names\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- 训练配置 -----------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/output/Qwen3-1.7B\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",  # 修复: eval_strategy -> evaluation_strategy\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=400,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"swanlab\",\n",
    "    run_name=\"qwen3-1.7B\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ----------------- 开始训练 -----------------\n",
    "trainer.train()\n",
    "\n",
    "# ----------------- 简单主观测试 -----------------\n",
    "test_df = pd.read_json(test_jsonl_new_path, lines=True)[:3]\n",
    "\n",
    "test_text_list = []\n",
    "for _, row in test_df.iterrows():\n",
    "    instruction = row[\"instruction\"]\n",
    "    input_value = row[\"input\"]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": input_value},\n",
    "    ]\n",
    "\n",
    "    response = predict(messages, model, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        f\"Question: {input_value}\\n\"\n",
    "        f\"LLM: {response}\\n\"\n",
    "    )\n",
    "\n",
    "    test_text_list.append(swanlab.Text(response_text))\n",
    "    print(response_text)\n",
    "\n",
    "swanlab.log({\"Prediction\": test_text_list})\n",
    "swanlab.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCMLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
